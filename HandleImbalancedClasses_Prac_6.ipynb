{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afce4579",
   "metadata": {},
   "source": [
    "**PTACTICAL 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813510f",
   "metadata": {},
   "source": [
    "**How to Handle Imbalanced Classes in Data Mining?**\n",
    "\n",
    "In data mining or ML, “imbalanced classes” is a familiar problem particularly occurring in classification when we have datasets with an unequal ratio of data points in each class.\n",
    "\n",
    "Training of model becomes much trickier as typical accuracy is no longer a reliable metric for measuring the performance of the model. Now if the number of data points in minority class is much less, then it may end up being completely ignored during training.\n",
    "\n",
    "**Problems with the imbalanced data**\n",
    "\n",
    "Unbalanced class distributions present a barrier, even though many machine learning algorithms work best when there are nearly equal numbers of samples in each class. A model may appear to have high accuracy in these situations if it primarily predicts the majority class. In such cases, having high accuracy becomes deceptive. Sadly, the minority class—which is frequently the main focus of model creation—is ignored by this strategy. In the event that 99% of the data pertains to the majority class, for example, simple classification models such as logistic regression or decision trees may find it difficult to recognize and precisely forecast occurrences from the minority class.\n",
    "\n",
    "**Class Imbalance Handling**\n",
    "\n",
    "Resampling, which modifies the sample distribution, is a frequently used technique for handling very unbalanced datasets. This can be accomplished by either over-sampling, which adds more examples from the minority class, or under-sampling, which removes samples from the majority class. One method for reducing the difficulties caused by severely skewed datasets is resampling, which balances the class distribution.\n",
    "\n",
    "Using strategies like over- and under-sampling to balance classes has advantages, but there are also disadvantages.\n",
    "\n",
    "A fundamental method of over-sampling is to replicate random records from the minority class, which may cause overfitting.\n",
    "\n",
    "In Up-sampling, samples from minority classes are randomly duplicated so as to achieve equivalence with the majority class. There are many methods used for achieving this.\n",
    "\n",
    "**1.Using Random Under-Sampling**\n",
    "\n",
    "When observations from the majority class are eliminated until the majority and minority classes are balanced, this is known as undersampling.\n",
    "\n",
    "Undersampling has advantages when working with large datasets, especially ones with millions of rows, but there is a risk that important information will be lost during the removal process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39bdb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    feature_1  feature_2  feature_3  feature_4  balance\n",
      "0   -1.053839  -1.027544  -0.329294   0.826007        1\n",
      "1    1.569317   1.306542  -0.239385  -0.331376        0\n",
      "2   -0.658926  -0.357633   0.723682  -0.628277        0\n",
      "3   -0.136856   0.460938   1.896911  -2.281386        0\n",
      "4   -0.048629   0.502301   1.778730  -2.171053        0\n",
      "..        ...        ...        ...        ...      ...\n",
      "95  -2.241820  -1.248690   2.357902  -2.009185        0\n",
      "96   0.573042   0.362054  -0.462814   0.341294        1\n",
      "97  -0.375121  -0.149518   0.588465  -0.575002        0\n",
      "98   1.042518   1.058239   0.461945  -0.984846        0\n",
      "99  -0.121203  -0.043997   0.204211  -0.203119        0\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "0    80\n",
      "1    80\n",
      "Name: balance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Importing scikit-learn, pandas library\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Making DataFrame having 100\n",
    "# dummy samples with 4 features\n",
    "# Divided in 2 classes in a ratio of 80:20\n",
    "X, y = make_classification(n_classes=2,\n",
    "                           weights=[0.8, 0.2],\n",
    "                           n_features=4,\n",
    "                           n_samples=100,\n",
    "                           random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature_1',\n",
    "                              'feature_2',\n",
    "                              'feature_3',\n",
    "                              'feature_4'])\n",
    "df['balance'] = y\n",
    "print(df)\n",
    "\n",
    "# Let df represent the dataset\n",
    "# Dividing majority and minority classes\n",
    "df_major = df[df.balance == 0]\n",
    "df_minor = df[df.balance == 1]\n",
    "\n",
    "# Upsampling minority class\n",
    "df_minor_sample = resample(df_minor,\n",
    "\n",
    "                           # Upsample with replacement\n",
    "                           replace=True,\n",
    "\n",
    "                           # Number to match majority class\n",
    "                           n_samples=80,\n",
    "                           random_state=42)\n",
    "\n",
    "# Combine majority and upsampled minority class\n",
    "df_sample = pd.concat([df_major, df_minor_sample])\n",
    "\n",
    "# Display count of data points in both class\n",
    "print(df_sample.balance.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7d45c",
   "metadata": {},
   "source": [
    "**Explanation :**\n",
    "\n",
    "Firstly, we’ll divide the data points from each class into separate DataFrames.\n",
    "\n",
    "After this, the minority class is resampled with replacement by setting the number of data points equivalent to that of the majority class.\n",
    "\n",
    "In the end, we’ll concatenate the original majority class DataFrame and up-sampled minority class DataFrame.\n",
    "\n",
    "**2.Using RandomOverSampler:**\n",
    "\n",
    "Oversampling is the process of adding more copies to the minority class. When dealing with constrained data resources, this approach is helpful. Overfitting and decreased generalization performance on the test set are potential drawbacks of oversampling, though.\n",
    "\n",
    "This can be done with the help of the RandomOverSampler method present in imblearn. This function randomly generates new data points belonging to the minority class with replacement (by default).\n",
    "\n",
    "Syntax: RandomOverSampler(sampling_strategy=’auto’, random_state=None, shrinkage=None)\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "sampling_strategy: Sampling Information for dataset.Some Values are- ‘minority’: only minority class ‘not minority’: all classes except minority class, ‘not majority’: all classes except majority class, ‘all’: all classes, ‘auto’: similar to ‘not majority’, Default value is ‘auto’\n",
    "\n",
    "random_state: Used for shuffling the data. If a positive non-zero number is given then it shuffles otherwise not. Default value is None.\n",
    "\n",
    "shrinkage: Parameter controlling the shrinkage. Values are: float: Shrinkage factor applied on all classes. dict: Every class will have a specific shrinkage factor. None: Shrinkage= 0. Default value is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442d9296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Over-Sampling: \n",
      "Samples in class 0:  80\n",
      "Samples in class 1:  20\n",
      "After Over-Sampling: \n",
      "Samples in class 0:  80\n",
      "Samples in class 1:  80\n"
     ]
    }
   ],
   "source": [
    "# Importing imblearn,scikit-learn library\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Making Dataset having 100\n",
    "# dummy samples with 4 features\n",
    "# Divided in 2 classes in a ratio of 80:20\n",
    "X, y = make_classification(n_classes=2,\n",
    "                           weights=[0.8, 0.2],\n",
    "                           n_features=4,\n",
    "                           n_samples=100,\n",
    "                           random_state=42)\n",
    "\n",
    "# Printing number of samples in\n",
    "# each class before Over-Sampling\n",
    "t = [(d) for d in y if d==0]\n",
    "s = [(d) for d in y if d==1]\n",
    "print('Before Over-Sampling: ')\n",
    "print('Samples in class 0: ',len(t))\n",
    "print('Samples in class 1: ',len(s))\n",
    "\n",
    "# Over Sampling Minority class\n",
    "OverS = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit predictor (x variable)\n",
    "# and target (y variable) using fit_resample()\n",
    "X_Over, Y_Over = OverS.fit_resample(X, y)\n",
    "\n",
    "# Printing number of samples in\n",
    "# each class after Over-Sampling\n",
    "t = [(d) for d in Y_Over if d==0]\n",
    "s = [(d) for d in Y_Over if d==1]\n",
    "print('After Over-Sampling: ')\n",
    "print('Samples in class 0: ',len(t))\n",
    "print('Samples in class 1: ',len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d1f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e343af",
   "metadata": {},
   "source": [
    "**3.Random Under-Sampling with Imblearn**\n",
    "There’s a library called imblearn, which is super helpful for fixing imbalanced datasets and making your models work better.\n",
    "\n",
    "One good thing in imblearn is RandomUnderSampler. It’s a quick and simple way to even out the data by randomly choosing some data from the classes we want to balance. Basically, it grabs a bunch of samples from the majority class (or classes) in a random way.\n",
    "\n",
    "Syntax: RandomUnderSampler(sampling_strategy=’auto’, random_state=None, replacement=False)\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "sampling_strategy: Sampling Information for dataset.\n",
    "\n",
    "random_state: Used for shuffling the data. If positive non zero number is given then it shuffles otherwise not. Default value is None.\n",
    "\n",
    "replacement: Implements resampling with or without replacement. Boolean type of value. Default value is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d9fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under-Sampling: \n",
      "Samples in class 0:  80\n",
      "Samples in class 1:  20\n",
      "After Under-Sampling: \n",
      "Samples in class 0:  20\n",
      "Samples in class 1:  20\n"
     ]
    }
   ],
   "source": [
    "# Importing imblearn library\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Making Dataset having\n",
    "# 100 dummy samples with 4 features\n",
    "# Divided in 2 classes in a ratio of 80:20\n",
    "X, y = make_classification(n_classes=2,\n",
    "                           weights=[0.8, 0.2],\n",
    "                           n_features=4,\n",
    "                           n_samples=100,\n",
    "                           random_state=42)\n",
    "\n",
    "# Printing number of samples\n",
    "# in each class before Under-Sampling\n",
    "t = [(d) for d in y if d == 0]\n",
    "s = [(d) for d in y if d == 1]\n",
    "print('Before Under-Sampling: ')\n",
    "print('Samples in class 0: ', len(t))\n",
    "print('Samples in class 1: ', len(s))\n",
    "\n",
    "# Down-Sampling majority class\n",
    "UnderS = RandomUnderSampler(random_state=42,\n",
    "                            replacement=True)\n",
    "\n",
    "# Fit predictor (x variable)\n",
    "# and target (y variable) using fit_resample()\n",
    "X_Under, Y_Under = UnderS.fit_resample(X, y)\n",
    "\n",
    "# Printing number of samples in\n",
    "# each class after Under-Sampling\n",
    "t = [(d) for d in Y_Under if d == 0]\n",
    "s = [(d) for d in Y_Under if d == 1]\n",
    "print('After Under-Sampling: ')\n",
    "print('Samples in class 0: ', len(t))\n",
    "print('Samples in class 1: ', len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cab203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
